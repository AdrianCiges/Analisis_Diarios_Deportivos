{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install newspaper3k\n",
    "# !pip install --upgrade beautifulsoup4\n",
    "# !pip install beautifulsoup4\n",
    "# %pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc152dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from newspaper import Article\n",
    "from datetime import datetime, date, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm \n",
    "from tqdm import tqdm\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22770dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver configuration\n",
    "opciones=Options()\n",
    "\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.headless=False    # si True, no aperece la ventana (headless=no visible)\n",
    "opciones.add_argument('--start-maximized')         # comienza maximizado\n",
    "#opciones.add_argument('user-data-dir=selenium')    # mantiene las cookies\n",
    "#opciones.add_extension('driver_folder/adblock.crx')       # adblocker\n",
    "opciones.add_argument('--incognito')\n",
    "\n",
    "PATH=ChromeDriverManager().install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad88b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redactores_SP(url):\n",
    "    try:\n",
    "        article=Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.authors\n",
    "    except:\n",
    "        return 'Desconocido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe752995",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia = str(date.today()).split('-')[2]\n",
    "mes = str(date.today()).split('-')[1]\n",
    "año = str(date.today()).split('-')[0][2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b06ace",
   "metadata": {},
   "source": [
    "## Periódicos Digitales a scrappear:\n",
    "### - Marca\n",
    "### - AS\n",
    "### - MundoDeportivo\n",
    "### - Sport\n",
    "### - EstadioDeportivo\n",
    "### - SuperDeporte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc686633",
   "metadata": {},
   "source": [
    "# Escreapeamos MARCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66294cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fechas(link):\n",
    "    try: \n",
    "        article=Article(link)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.publish_date.date()\n",
    "    except:\n",
    "        return 'Desconocido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c22928d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s][Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      " 74%|███████▍  | 48/65 [00:05<00:01, 13.49it/s][Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    5.2s\n",
      "100%|██████████| 65/65 [00:05<00:00, 11.00it/s]\n",
      "[Parallel(n_jobs=6)]: Done  65 out of  65 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>web</th>\n",
       "      <th>titular</th>\n",
       "      <th>redactor</th>\n",
       "      <th>comentarios</th>\n",
       "      <th>link</th>\n",
       "      <th>noticia</th>\n",
       "      <th>seccion</th>\n",
       "      <th>fecha_publicacion</th>\n",
       "      <th>fecha_actual</th>\n",
       "      <th>desactualizacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marca</td>\n",
       "      <td>Revolución en Inglaterra e Italia: ¿qué está p...</td>\n",
       "      <td>Jon Prada E Irati Prat</td>\n",
       "      <td>103</td>\n",
       "      <td>https://www.marca.com/futbol/futbol-internacio...</td>\n",
       "      <td>Noticia</td>\n",
       "      <td>futbol</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marca</td>\n",
       "      <td>Alves cayó en la trampa de la policía: así fue...</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>Sin Comentarios</td>\n",
       "      <td>https://us.marca.com/actualidad/2023/01/24/63c...</td>\n",
       "      <td>Noticia</td>\n",
       "      <td>actualidad</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marca</td>\n",
       "      <td>Danjuma lo tenía hecho con el Everton... ¡pero...</td>\n",
       "      <td>Aitana Sánchez</td>\n",
       "      <td>84</td>\n",
       "      <td>https://www.marca.com/futbol/mercado-fichajes/...</td>\n",
       "      <td>Noticia</td>\n",
       "      <td>futbol</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marca</td>\n",
       "      <td>La pieza clave para la llegada de Bellingham a...</td>\n",
       "      <td>Juan Ignacio García-Ochoa</td>\n",
       "      <td>197</td>\n",
       "      <td>https://www.marca.com/futbol/real-madrid/2023/...</td>\n",
       "      <td>Noticia</td>\n",
       "      <td>futbol</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marca</td>\n",
       "      <td>Reine-Adélaïde pasa el reconocimiento médico.....</td>\n",
       "      <td>A. Fdez.</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.marca.com/futbol/sevilla/2023/01/2...</td>\n",
       "      <td>Noticia</td>\n",
       "      <td>futbol</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     web                                            titular  \\\n",
       "0  Marca  Revolución en Inglaterra e Italia: ¿qué está p...   \n",
       "1  Marca  Alves cayó en la trampa de la policía: así fue...   \n",
       "2  Marca  Danjuma lo tenía hecho con el Everton... ¡pero...   \n",
       "3  Marca  La pieza clave para la llegada de Bellingham a...   \n",
       "4  Marca  Reine-Adélaïde pasa el reconocimiento médico.....   \n",
       "\n",
       "                    redactor      comentarios  \\\n",
       "0     Jon Prada E Irati Prat             103    \n",
       "1                Desconocido  Sin Comentarios   \n",
       "2             Aitana Sánchez              84    \n",
       "3  Juan Ignacio García-Ochoa             197    \n",
       "4                   A. Fdez.              30    \n",
       "\n",
       "                                                link  noticia     seccion  \\\n",
       "0  https://www.marca.com/futbol/futbol-internacio...  Noticia      futbol   \n",
       "1  https://us.marca.com/actualidad/2023/01/24/63c...  Noticia  actualidad   \n",
       "2  https://www.marca.com/futbol/mercado-fichajes/...  Noticia      futbol   \n",
       "3  https://www.marca.com/futbol/real-madrid/2023/...  Noticia      futbol   \n",
       "4  https://www.marca.com/futbol/sevilla/2023/01/2...  Noticia      futbol   \n",
       "\n",
       "  fecha_publicacion fecha_actual desactualizacion  \n",
       "0        2023-01-24   2023-01-24                0  \n",
       "1        2023-01-24   2023-01-24                0  \n",
       "2        2023-01-24   2023-01-24                0  \n",
       "3        2023-01-24   2023-01-24                0  \n",
       "4        2023-01-24   2023-01-24                0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marca = 'https://www.marca.com/'\n",
    "\n",
    "html = req.get(marca).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('div', class_='ue-c-cover-content__body')\n",
    "titulares_raw = soup.find_all('h2', class_='ue-c-cover-content__headline')\n",
    "titulares = [e.text for e in titulares_raw]\n",
    "\n",
    "redactores_raw = [e.find_all('span', class_='ue-c-cover-content__byline-name') for e in bodies]\n",
    "redactores = [e[0].text.split('\\n')[1].strip().title() if len(e)>0 else 'Desconocido' for e in redactores_raw]\n",
    "\n",
    "links = [e.find('a', class_='ue-c-cover-content__link').attrs['href'] for e in bodies]\n",
    "\n",
    "comments_raw = [e.find('a', class_='ue-c-cover-content__comments') for e in bodies]\n",
    "comments = [e.text.split('comentarios')[0] if e is not None else 'Sin Comentarios' for e in comments_raw]\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_MARCA = pd.DataFrame()\n",
    "df_MARCA['web'] = ['Marca' for i in range(len(titulares))]\n",
    "df_MARCA['titular'] = titulares\n",
    "df_MARCA['redactor'] = redactores\n",
    "df_MARCA['comentarios'] = comments\n",
    "df_MARCA['link'] = links\n",
    "df_MARCA['noticia'] = ['Video' if 'videos.marca' in e else 'Noticia' for e in df_MARCA['link']] # Filtramos noticias válidas\n",
    "try:\n",
    "    df_MARCA['seccion'] = [e.split('.com/')[1].split('/')[0] for e in df_MARCA['link']] # Añadimos columna de sección\n",
    "except:\n",
    "    df_MARCA['seccion'] = 'Desconocido'\n",
    "df_MARCA['seccion'] = ['video' if e == 'v' else 'Desconocido' if e == '' else e for e in df_MARCA['seccion']] # corregimos detalles \n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_MARCA['fecha_publicacion'] = fechas\n",
    "\n",
    "df_MARCA['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_MARCA['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_MARCA['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_MARCA['desactualizacion'] = desactualizacion\n",
    "\n",
    "# df_MARCA = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_MARCA.to_excel(f'./MARCA_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_MARCA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63b131",
   "metadata": {},
   "source": [
    "#### Probamos todo con Librería NewsPapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fb339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marca = 'https://www.marca.com/'\n",
    "\n",
    "# html = req.get(marca).text\n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# links = [e.find('a', class_='ue-c-cover-content__link').attrs['href'] for e in bodies]\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# redactores = []\n",
    "# titulos = []\n",
    "\n",
    "# for e in links:\n",
    "#     article=Article(e)\n",
    "#     article.download()\n",
    "#     article.parse()\n",
    "    \n",
    "#     redactores.append(article.authors)\n",
    "#     titulos.append(article.title)\n",
    "    \n",
    "    \n",
    "# comments_raw = [e.find('a', class_='ue-c-cover-content__comments') for e in bodies]\n",
    "# comments = [e.text.split('comentarios')[0] if e is not None else 'Sin Comentarios' for e in comments_raw]\n",
    "    \n",
    "# df['web'] = ['Marca' for i in range(len(titulos))]\n",
    "# df['titular'] = titulos\n",
    "# df['redactor'] = redactores\n",
    "# df['comentarios'] = comments\n",
    "# df['link'] = links\n",
    "# df['noticia'] = ['Video' if 'videos.marca' in e else 'Noticia' for e in df['link']] # Filtramos noticias válidas\n",
    "# df['seccion'] = [e.split('.com/')[1].split('/')[0] for e in df['link']] # Añadimos columna de sección\n",
    "# df['seccion'] = ['video' if e == 'v' else 'Desconocido' if e == '' else e for e in df['seccion']] # corregimos detalles\n",
    "\n",
    "# # Añadimos la fecha\n",
    "# fechas = []\n",
    "# for e in links:\n",
    "#     article=Article(e)\n",
    "#     article.download()\n",
    "#     article.parse()\n",
    "#     fechas.append(article.publish_date)\n",
    "\n",
    "# df['fecha'] = [e if e is None else datetime.strptime(str(e).split()[0], '%Y-%m-%d').date() for e in fechas]\n",
    "\n",
    "# df\n",
    "\n",
    "# Interesantes opciones, pero casi prefiero el ajuste manual porque los titulares con los de portada, los redactores los que\n",
    "# firman, debo usar BS4 para los comentarios... nos quedamos con BS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40085ba0",
   "metadata": {},
   "source": [
    "# Escreapeamos AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9ee5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_AS(url):\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "        driver.get(url) \n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.find_elements(By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]')[0].click() # Aceptamos cookies\n",
    "\n",
    "        return int(driver.find_elements(By.XPATH, '//*[@id=\"top-num-comments\"]')[0].text) # Nº de comentarios\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return 0\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titulares cazados\n",
      "Links cazados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/139 [00:00<?, ?it/s][Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      " 60%|██████    | 84/139 [00:02<00:02, 25.25it/s][Parallel(n_jobs=6)]: Done  64 tasks      | elapsed:    2.7s\n",
      "100%|██████████| 139/139 [00:05<00:00, 27.09it/s]\n",
      "[Parallel(n_jobs=6)]: Done 128 out of 139 | elapsed:    5.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=6)]: Done 139 out of 139 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redactores cazados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/139 [00:00<?, ?it/s][Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      " 35%|███▍      | 48/139 [07:09<15:10, 10.00s/it][Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  7.3min\n",
      " 82%|████████▏ | 114/139 [16:57<02:55,  7.03s/it]"
     ]
    }
   ],
   "source": [
    "AS = 'https://as.com/'\n",
    "\n",
    "html = req.get(AS).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('article')\n",
    "\n",
    "titulares_raw = soup.find_all('h2', class_='s__tl')\n",
    "titulares = [e.text for e in titulares_raw]\n",
    "print('Titulares cazados')\n",
    "\n",
    "links = [e.find('a').attrs['href'] for e in bodies]\n",
    "print('Links cazados')\n",
    "\n",
    "# redactores_raw = [e.find_all('span', class_='s__au') for e in bodies]\n",
    "# redactores = [e[0].text.title() for e in redactores_raw]\n",
    "\n",
    "redactores = Parallel(n_jobs=6, verbose=True)(delayed(get_redactores_SP)(link) for link in tqdm(links))\n",
    "\n",
    "print('Redactores cazados')\n",
    "\n",
    "\n",
    "comments = Parallel(n_jobs=6, verbose=True)(delayed(get_comments_AS)(link) for link in tqdm(links))\n",
    "print('Comentarios cazados')\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_AS = pd.DataFrame()\n",
    "df_AS['web'] = ['AS' for i in range(len(titulares))]\n",
    "df_AS['titular'] = titulares\n",
    "df_AS['redactor'] = redactores\n",
    "df_AS['comentarios'] = comments\n",
    "df_AS['link'] = links\n",
    "df_AS['noticia'] = ['Video' if '/videos/' in e else 'Noticia' for e in df_AS['link']] # Filtramos noticias válidas\n",
    "\n",
    "seccion = []\n",
    "for e in links:\n",
    "    if 'futbol' in e or 'mundial' in e or 'tikitaka' in e:\n",
    "        seccion.append('Futbol')\n",
    "    elif 'tenis' in e:\n",
    "        seccion.append('Tenis')\n",
    "    elif 'motor' in e:\n",
    "        seccion.append('motor')\n",
    "    elif '/videos/' in e:\n",
    "        seccion.append(e.split('/videos/')[1].split('/')[0])\n",
    "    elif '.com/' in e:\n",
    "        seccion.append(e.split('.com/')[1].split('/')[0])\n",
    "    else:\n",
    "        seccion.append('Desconocido')\n",
    "        \n",
    "df_AS['seccion'] = seccion\n",
    "print('¡Creado!')\n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "print('Añadiendo fechas...')\n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_AS['fecha_publicacion'] = fechas\n",
    "\n",
    "df_AS['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "print('Fechas añadidas!')\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_AS['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_AS['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_AS['desactualizacion'] = desactualizacion\n",
    "\n",
    "print('¡ÉXITO!')\n",
    "\n",
    "# df_AS = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_AS.to_excel(f'./AS_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_AS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f8990",
   "metadata": {},
   "source": [
    "# Escreapeamos MundoDeportivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(url):\n",
    "    try:\n",
    "        article=Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title\n",
    "    except:\n",
    "        return 'Desconocido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_MD(url):\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "        driver.get(url) \n",
    "        time.sleep(1)\n",
    "        driver.find_elements(By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]')[0].click() # Aceptamos cookies\n",
    "        return int(driver.find_elements(By.XPATH, '//*[@id=\"main-container\"]/div[3]/article/div[1]/div[2]/div[2]/div[2]/div[2]/div[2]/a/span')[0].text) # Nº de comentarios\n",
    "    \n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD = 'https://www.mundodeportivo.com/'\n",
    "\n",
    "html = req.get(MD).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('article')\n",
    "\n",
    "verif = False\n",
    "\n",
    "while verif == False:\n",
    "\n",
    "    print('Cazando Titulares...')\n",
    "    titulares = Parallel(n_jobs=6, verbose=True)(delayed(get_titles)(link) for link in tqdm(links))\n",
    "    print('¡Cazados Titulares!')\n",
    "\n",
    "    # Redactores NO EN PORTADA\n",
    "\n",
    "    url_base = 'https://www.mundodeportivo.com/'\n",
    "\n",
    "    links = []\n",
    "    for e in bodies:\n",
    "        try:\n",
    "            links.append(e.find('a').attrs['href'])\n",
    "        except:\n",
    "            links.append('Desconocido')\n",
    "    links = [e if 'http' in e else url_base+e for e in links]\n",
    "    print(f'Cazados {len(links)} links')\n",
    "\n",
    "    print('Cazando Redactores')\n",
    "    redactores = Parallel(n_jobs=6, verbose=True)(delayed(get_redactores_MD)(link) for link in tqdm(links))\n",
    "    \n",
    "    verif = len(links) == len(titulares) == len(redactores)\n",
    "        \n",
    "print('Cazando Comments')\n",
    "comments = Parallel(n_jobs=6, verbose=True)(delayed(get_comments_MD)(link) for link in tqdm(links))\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_MD = pd.DataFrame()\n",
    "df_MD['web'] = ['MD' for i in range(len(titulares))]\n",
    "df_MD['titular'] = titulares\n",
    "df_MD['redactor'] = redactores\n",
    "df_MD['comentarios'] = comments\n",
    "df_MD['link'] = links\n",
    "df_MD['noticia'] = ['Video' if '/videos/' in e else 'Noticia' for e in df_MD['link']] # Filtramos noticias válidas\n",
    "df_MD['seccion'] = [e.split('.com/')[1].split('/')[0] for e in df_MD['link']] # Añadimos columna de sección\n",
    "print('¡Creado!')\n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "print('Añadiendo fechas...')\n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_MD['fecha_publicacion'] = fechas\n",
    "\n",
    "df_MD['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "print('Fechas añadidas!')\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_MD['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_MD['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_MD['desactualizacion'] = desactualizacion\n",
    "\n",
    "print('¡ÉXITO!')\n",
    "\n",
    "# df_MD = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_MD.to_excel(f'./MundoDeportivo_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_MD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc00ef7",
   "metadata": {},
   "source": [
    "# Escreapeamos SPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca527b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_SP(url):\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "        driver.get(url) \n",
    "        time.sleep(1)\n",
    "        driver.find_elements(By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]')[0].click() # Aceptamos cookies\n",
    "        return int(driver.find_elements(By.XPATH, '//*[@id=\"infinite-scroll-details-item-1\"]/div[2]/article/div[2]/div[1]/div[1]/div/div[2]/div[1]/div[2]/p/a[4]/span')[0].text) # Nº de comentarios\n",
    "        driver.quit()\n",
    "    \n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24892cf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SP = 'https://www.sport.es/es/'\n",
    "\n",
    "html = req.get(SP).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('article')\n",
    "\n",
    "# Títulos con fallos por tildes. Cambiamos estrategia: Usamos librería Newspaper. Necesitamos links primero.\n",
    "\n",
    "links = [e.find('a').attrs['href'] for e in bodies]\n",
    "print('Cazados Links')\n",
    "\n",
    "print('Cazando Titulares...')\n",
    "titulares = Parallel(n_jobs=6, verbose=True)(delayed(get_titles)(link) for link in tqdm(links))\n",
    "print('¡Cazados Titulares!')\n",
    "\n",
    "redactores = Parallel(n_jobs=6, verbose=True)(delayed(get_redactores_SP)(link) for link in tqdm(links))\n",
    "redactores = [e[0].title() if len(e)>0 else 'Desconocido' for e in redactores]\n",
    "\n",
    "# print('Canzando Comentarios')\n",
    "# comments = Parallel(n_jobs=6, verbose=True)(delayed(get_comments_SP)(link) for link in tqdm(links))\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_SP = pd.DataFrame()\n",
    "df_SP['web'] = ['SPORT' for i in range(len(titulares))]\n",
    "df_SP['titular'] = titulares\n",
    "df_SP['redactor'] = redactores\n",
    "df_SP['comentarios'] = ['Desconocido' for i in range(len(titulares))]\n",
    "df_SP['link'] = links\n",
    "df_SP['noticia'] = ['Video' if '/videos/' in e else 'Noticia' for e in df_SP['link']] # Filtramos noticias válidas\n",
    "seccion = []\n",
    "for e in links:\n",
    "    if 'futbol' in e or 'barca' in e or 'madrid' in e or 'seleccion' in e or 'mercado-fichajes' in e or 'mundial' in e:\n",
    "        seccion.append('Futbol')\n",
    "    elif 'psg' in e or 'espanyol' in e or 'girona' in e or 'fuera-de-juego' in e or 'segunda-division' in e or 'rayo-vallecano' in e:\n",
    "        seccion.append('Futbol')\n",
    "    elif 'euroliga' in e or 'nba' in e:\n",
    "        seccion.append('baloncesto')\n",
    "    elif '/noticias/' in e:\n",
    "        seccion.append(e.split('/noticias/')[1].split('/')[0])\n",
    "    elif '/videos/' in e:\n",
    "        seccion.append(e.split('/videos/')[1].split('/')[0])\n",
    "    elif '/es/' in e:\n",
    "        seccion.append(e.split('/es/')[1].split('/')[0])\n",
    "    elif '.es/' in e:\n",
    "        seccion.append(e.split('.es/')[1].split('/')[0])\n",
    "    elif '.com/' in e:\n",
    "        seccion.append(e.split('.com/')[1].split('/')[0])\n",
    "    else:\n",
    "        seccion.append('Otros')\n",
    "seccion = [e.replace('-',' ').title() for e in seccion]\n",
    "df_SP['seccion'] = seccion\n",
    "print('¡Creado!')\n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "print('Añadiendo fechas...')\n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_SP['fecha_publicacion'] = fechas\n",
    "\n",
    "df_SP['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "print('Fechas añadidas!')\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_SP['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_SP['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_SP['desactualizacion'] = desactualizacion\n",
    "\n",
    "print('¡ÉXITO!')\n",
    "\n",
    "# df_MD = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_SP.to_excel(f'./SPORT_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_SP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d10ea",
   "metadata": {},
   "source": [
    "# Escreapeamos EstadioDeportivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d821ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redactores_ED(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "        driver.get(url) \n",
    "        time.sleep(2)\n",
    "        driver.find_elements(By.XPATH, '//*[@id=\"acceptAllMain\"]')[0].click() # Aceptamos cookies\n",
    "        redactor = driver.find_elements(By.XPATH, '/html/body/div[2]/span[2]/span[2]/span[1]/span[1]/span[2]/span[1]/a[1]')[0].text.title()\n",
    "        driver.quit()\n",
    "        return redactor\n",
    "    except:\n",
    "        try:\n",
    "            driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "            driver.get(url) \n",
    "            time.sleep(2)\n",
    "            driver.find_elements(By.XPATH, '//*[@id=\"acceptAllMain\"]')[0].click() # Aceptamos cookies\n",
    "            redactor = driver.find_elements(By.XPATH, '/html/body/div[2]/span[2]/span[2]/span[1]/span[2]/span[2]/span[1]/a[1]')[0].text.title()\n",
    "            driver.quit()\n",
    "            return redactor\n",
    "        except:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "                driver.get(url) \n",
    "                time.sleep(2)\n",
    "                driver.find_elements(By.XPATH, '//*[@id=\"acceptAllMain\"]')[0].click() # Aceptamos cookies\n",
    "                redactor = driver.find_elements(By.XPATH, '/html/body/div[2]/span[2]/span[2]/span[1]/span[1]/span/a')[0].text.title()\n",
    "                driver.quit()\n",
    "                return redactor\n",
    "            except:\n",
    "                return 'Desconocido'\n",
    "            \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d7299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ED = 'https://www.estadiodeportivo.com/'\n",
    "\n",
    "html = req.get(ED).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('article')\n",
    "\n",
    "links = [e.find('a').attrs['href'] for e in bodies]\n",
    "print('Cazados Links')\n",
    "\n",
    "print('Cazando Titulares...')\n",
    "titulares = Parallel(n_jobs=6, verbose=True)(delayed(get_titles)(link) for link in tqdm(links))\n",
    "print('¡Cazados Titulares!')\n",
    "\n",
    "redactores = Parallel(n_jobs=6, verbose=True)(delayed(get_redactores_ED)(link) for link in tqdm(links))\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_ED = pd.DataFrame()\n",
    "df_ED['web'] = ['EstadioDeportivo' for i in range(len(titulares))]\n",
    "df_ED['titular'] = titulares\n",
    "df_ED['redactor'] = redactores\n",
    "df_ED['comentarios'] = ['Desconocido' for i in range(len(titulares))]\n",
    "df_ED['link'] = links\n",
    "df_ED['noticia'] = ['Video' if '/videos/' in e else 'Noticia' for e in df_ED['link']] # Filtramos noticias válidas\n",
    "df_ED['seccion'] = [e.split('.com/')[1].split('/')[0] for e in links]\n",
    "\n",
    "print('¡Creado!')\n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "print('Añadiendo fechas...')\n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_ED['fecha_publicacion'] = fechas\n",
    "\n",
    "df_ED['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "print('Fechas añadidas!')\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_ED['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_ED['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_ED['desactualizacion'] = desactualizacion\n",
    "\n",
    "print('¡ÉXITO!')\n",
    "\n",
    "# df_MD = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_ED.to_excel(f'./EstadioDeportivo_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_ED.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c3f06",
   "metadata": {},
   "source": [
    "# Escreapeamos SuperDeporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e220e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redactores_Super(url):\n",
    "    try:\n",
    "        article=Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.authors[0]\n",
    "    except:\n",
    "        return 'Desconocido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a212718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_Super(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(PATH, options=opciones) #Para que abra la pestaña de Chrome\n",
    "        driver.get(url) \n",
    "        time.sleep(1)\n",
    "        driver.find_elements(By.XPATH, '/html/body/div[1]/div/div/div/div/div/div[2]/button[2]')[0].click() # Aceptamos cookies\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        return int(driver.find_elements(By.XPATH, '/html/body/main/div[5]/div/div/div/article/div/div')[0].text.split()[0]) # Nº de comentarios\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb31380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUPER = 'https://www.superdeporte.es/'\n",
    "\n",
    "html = req.get(SUPER).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bodies = soup.find_all('article')\n",
    "\n",
    "links = []\n",
    "for e in bodies:\n",
    "    try:\n",
    "        if 'https://' in e.find('a').attrs['href']:\n",
    "            links.append(e.find('a').attrs['href'].strip())\n",
    "        else:\n",
    "            links.append('https://www.superdeporte.es/'+e.find('a').attrs['href'].strip())\n",
    "    except:\n",
    "        links.append('Sin Link')\n",
    "print('Cazados Links')\n",
    "\n",
    "print('Cazando Titulares...')\n",
    "titulares = Parallel(n_jobs=6, verbose=True)(delayed(get_titles)(link) for link in tqdm(links))\n",
    "print('¡Cazados Titulares!')\n",
    "\n",
    "redactores = Parallel(n_jobs=6, verbose=True)(delayed(get_redactores_Super)(link) for link in tqdm(links))\n",
    "print('Redactores cazados')\n",
    "\n",
    "# comments = Parallel(n_jobs=6, verbose=True)(delayed(get_comments_Super)(link) for link in tqdm(links))\n",
    "\n",
    "print('Creando DataFrame...')\n",
    "df_Super = pd.DataFrame()\n",
    "df_Super['web'] = ['SuperDeporte' for i in range(len(titulares))]\n",
    "df_Super['titular'] = titulares\n",
    "df_Super['redactor'] = redactores\n",
    "df_Super['comentarios'] = ['Desconocido' for i in range(len(titulares))]\n",
    "df_Super['link'] = links\n",
    "df_Super['noticia'] = ['Video' if '/videos/' in e else 'Noticia' for e in df_Super['link']] # Filtramos noticias válidas\n",
    "seccion = []\n",
    "for e in links:\n",
    "    try:\n",
    "        seccion.append(e.split('.es//')[1].split('/')[0])\n",
    "    except:\n",
    "        seccion.append('Otros')\n",
    "df_Super['seccion'] = seccion\n",
    "print('¡Creado!')\n",
    "\n",
    "# Añadimos las fechas de publicación    \n",
    "print('Añadiendo fechas...')\n",
    "fechas = Parallel(n_jobs=6, verbose=True)(delayed(add_fechas)(link) for link in tqdm(links))\n",
    "\n",
    "df_Super['fecha_publicacion'] = fechas\n",
    "\n",
    "df_Super['fecha_actual'] = [date.today() for e in range(len(titulares))]\n",
    "print('Fechas añadidas!')\n",
    "\n",
    "# Añadimos margen de días entre actualidad y fecha de la noticia\n",
    "desactualizacion = []\n",
    "for i,e in enumerate(df_Super['fecha_publicacion']):\n",
    "    if e != 'Desconocido':\n",
    "        fecha1 = e\n",
    "        fecha2 = df_Super['fecha_actual'][i]\n",
    "        dias = (fecha2 - fecha1) / timedelta(days=1)\n",
    "        desactualizacion.append(int(dias))\n",
    "    else:\n",
    "        desactualizacion.append(e)\n",
    "        \n",
    "df_Super['desactualizacion'] = desactualizacion\n",
    "\n",
    "print('¡ÉXITO!')\n",
    "\n",
    "# df_MD = HTML(df.to_html(render_links=True)) # Hacemos los links clicables\n",
    "\n",
    "df_Super.to_excel(f'./Super_{dia}_{mes}_{año}.xlsx')\n",
    "\n",
    "df_Super.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
